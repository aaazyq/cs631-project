{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_elephas_deep_learning_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5g36yct55Ng"
      },
      "source": [
        "reference: https://github.com/maxpumperla/elephas/blob/master/examples/Spark_ML_Pipeline.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbPhbxqXmlHg"
      },
      "source": [
        "## INSTALL PYSPARK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oqHdlpDY0bU"
      },
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfNezuIyZKwI",
        "outputId": "1aa8980e-5033-4c00-a491-176c46165ee8"
      },
      "source": [
        "from __future__ import print_function\n",
        "print(sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=YourTest>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Y2X2TZZuOj"
      },
      "source": [
        "# ! pip install elephas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbd4Vlb2yd6L",
        "outputId": "c025db79-48ec-489a-aad8-fe74953b17e2"
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from keras import layers, regularizers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaTEF9cnaV9K"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "sql_context = SQLContext(sc)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTejcnb3mvMu"
      },
      "source": [
        "##BUILD TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOZBXNSWjDy7"
      },
      "source": [
        "datapath = \"./\"\n",
        "csv_file = \"data_cleaned_shuffled.csv\"\n",
        "data = pd.read_csv(datapath + csv_file)\n",
        "data[\"text_clean\"] = data['text_cleaned_string'].apply(lambda x: str(x).split())\n",
        "max_fatures = 10000 \n",
        "tokenizer = Tokenizer(num_words=max_fatures)\n",
        "tokenizer.fit_on_texts(data['text_clean'].values)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbfP_wy6jGcT"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(data['text_clean'].values)\n",
        "X = pad_sequences(X, maxlen = 32) \n",
        "\n",
        "Y = pd.get_dummies(data['category']).values \n",
        "X_train_total, X_test, Y_train_total, Y_test = train_test_split(X, Y, test_size = 0.05, random_state = 42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO0XvoG9ropl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb97666-c311-403a-a80d-61f07ca3dd56"
      },
      "source": [
        "# create train and test dataset\n",
        "\n",
        "df_train = pd.DataFrame(X_train_total)\n",
        "df_train[\"label\"] = np.argmax(Y_train_total, axis = 1)\n",
        "df_train.to_csv(\"df_train.csv\", index = False)\n",
        "\n",
        "\n",
        "df_test = pd.DataFrame(X_test)\n",
        "df_test[\"label\"] = np.argmax(Y_test, axis = 1)\n",
        "df_test.to_csv(\"df_test.csv\", index = False)\n",
        "\n",
        "len(df_train), len(df_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57109, 3006)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jJT403cmgEk"
      },
      "source": [
        "## CREATE RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2K5ixQ6bsro"
      },
      "source": [
        "datapath = \"./\"\n",
        "csv_file = \"df_train.csv\"\n",
        "data = sc.textFile(datapath + csv_file)\n",
        "header = data.first()\n",
        "\n",
        "def str_to_int(x):\n",
        "    return [int(i) for i in x]\n",
        "\n",
        "data_rdd = data.filter(lambda x:x!= header).map(lambda line: line.split(',')).\\\n",
        "    map(lambda x: str_to_int(x)).\\\n",
        "    map(lambda x: (Vectors.dense(np.asarray(x[:-1]).astype(np.float32)), x[-1]))\n",
        "\n",
        "\n",
        "train_df = sql_context.createDataFrame(data_rdd, ['features', 'category'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj3btF-FvE2H"
      },
      "source": [
        "datapath = \"./\"\n",
        "csv_file = \"df_test.csv\"\n",
        "data = sc.textFile(datapath + csv_file)\n",
        "header = data.first()\n",
        "\n",
        "def str_to_int(x):\n",
        "    return [int(i) for i in x]\n",
        "\n",
        "data_rdd = data.filter(lambda x:x!= header).map(lambda line: line.split(',')).\\\n",
        "    map(lambda x: str_to_int(x)).\\\n",
        "    map(lambda x: (Vectors.dense(np.asarray(x[:-1]).astype(np.float32)), x[-1]))\n",
        "\n",
        "\n",
        "test_df = sql_context.createDataFrame(data_rdd, ['features', 'category'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAR9K1brv6Ef",
        "outputId": "bba55a68-cdc5-4e24-fcbc-2a0d91909814"
      },
      "source": [
        "test_df.show(5, truncate = False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "|features                                                                                                                                                    |category|\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,9.0,32.0]                          |2       |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,219.0,3.0,1607.0,1131.0,208.0,1514.0,793.0,168.0,529.0,63.0]       |1       |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,910.0,470.0,367.0,679.0,23.0,1583.0,2.0,25.0]              |2       |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,59.0,95.0,107.0,70.0,45.0,15.0,347.0,1159.0,1074.0,852.0,39.0,49.0,230.0,1067.0]   |0       |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,331.0,151.0,2898.0,416.0,830.0,1386.0,56.0,1902.0,6469.0,2310.0,1842.0]|0       |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lOEeXIrwkeb",
        "outputId": "9a9780d9-1116-428f-ce72-04aac0f49cd9"
      },
      "source": [
        "# make sure 'features' is vector type\n",
        "test_df.printSchema()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- category: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKoZm-CuvXEL"
      },
      "source": [
        "# ========== convert the label to index\n",
        "# don't need this part this time\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"index_category\")\n",
        "fitted_indexer = string_indexer.fit(train_df)\n",
        "indexed_df = fitted_indexer.transform(train_df)\n",
        "\n",
        "# =========== scaling =============\n",
        "# don't need this part this time\n",
        "\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
        "fitted_scaler = scaler.fit(indexed_df)\n",
        "scaled_df = fitted_scaler.transform(indexed_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0RfCAwGoKKK"
      },
      "source": [
        "## Build Model and ElephasEstimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izJi6ltkyNic",
        "outputId": "67d4e7e1-3741-4f5f-9d7a-3ed4dcdceada"
      },
      "source": [
        "# simple lstm\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from keras import layers, regularizers\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_fatures, embed_dim,input_length = 32))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.LSTM(lstm_out, dropout=0.5, recurrent_dropout=0.2))\n",
        "model.add(tf.keras.layers.Dense(3,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "save_best_model = tf.keras.callbacks.ModelCheckpoint('best_model_total.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "print(model.summary()) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 32, 128)           1280000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 591       \n",
            "=================================================================\n",
            "Total params: 1,535,391\n",
            "Trainable params: 1,535,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy93FfoayY2O",
        "outputId": "1b94593f-7730-434e-b2ad-4da5e5452763"
      },
      "source": [
        "from elephas.ml_model import ElephasEstimator\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "\n",
        "adam = optimizers.Adam(lr=0.01)\n",
        "opt_conf = optimizers.serialize(adam)\n",
        "\n",
        "# Initialize SparkML Estimator and set all relevant properties\n",
        "estimator = ElephasEstimator()\n",
        "estimator.setFeaturesCol(\"features\")             # These two come directly from pyspark,\n",
        "estimator.setLabelCol(\"category\")        \n",
        "estimator.set_keras_model_config(model.to_yaml())       # Provide serialized Keras model\n",
        "estimator.set_categorical_labels(True)\n",
        "estimator.set_nb_classes(3) \n",
        "estimator.set_num_workers(4)  \n",
        "estimator.set_epochs(20) \n",
        "estimator.set_batch_size(512)\n",
        "estimator.set_verbosity(1) \n",
        "estimator.set_validation_split(0.15)\n",
        "estimator.set_optimizer_config(opt_conf)\n",
        "estimator.set_mode(\"synchronous\")\n",
        "estimator.set_loss(\"categorical_crossentropy\")\n",
        "estimator.set_metrics(['acc'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElephasEstimator_fb9f7bad3c2d"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_Masrp21kG"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[estimator]) "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMfF1EKv3bG1",
        "outputId": "6bd9e9c9-c225-49a8-89f8-a077697e879c"
      },
      "source": [
        "fitted_pipeline = pipeline.fit(train_df) # Fit model to data \n",
        "# it will take fairly long time to train"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Fit model\n",
            ">>> Synchronous training complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_39_Dyn3KfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9261027-68a5-4e31-a2bc-ffd11fc904b2"
      },
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# prediction = fitted_pipeline.transform(train_df) # Evaluate on train data.\n",
        "prediction = fitted_pipeline.transform(test_df) # <-- The same code evaluates test data.\n",
        "pnl = prediction.select(\"category\", \"prediction\")\n",
        "pnl.show(10, truncate=False) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----------------------------------------------------------------+\n",
            "|category|prediction                                                       |\n",
            "+--------+-----------------------------------------------------------------+\n",
            "|2       |[0.04426686838269234, 0.16747823357582092, 0.7882549166679382]   |\n",
            "|1       |[0.9730930328369141, 0.019218582659959793, 0.007688305340707302] |\n",
            "|2       |[0.09128804504871368, 0.6165599226951599, 0.2921519875526428]    |\n",
            "|0       |[0.999300479888916, 2.9267198988236487E-4, 4.0680859819985926E-4]|\n",
            "|0       |[0.984204113483429, 0.004681208170950413, 0.011114677414298058]  |\n",
            "|0       |[0.3636516034603119, 0.49921563267707825, 0.13713277876377106]   |\n",
            "|1       |[0.6880679726600647, 0.2330729365348816, 0.07885913550853729]    |\n",
            "|0       |[0.9989369511604309, 3.6318288766779006E-4, 6.998337339609861E-4]|\n",
            "|0       |[0.7662630677223206, 0.13533107936382294, 0.09840583801269531]   |\n",
            "|1       |[0.04812038689851761, 0.8488356471061707, 0.10304394364356995]   |\n",
            "+--------+-----------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KYCQc77ndrn",
        "outputId": "5b481215-a46e-4da5-943b-25df884f4120"
      },
      "source": [
        "prediction_and_label = pnl.rdd.map((lambda row: (float(row.category), float(np.argmax(row.prediction)))))\n",
        "metrics = MulticlassMetrics(prediction_and_label)\n",
        "print(metrics.precision()) "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6689953426480373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUOqoSRhxlGW",
        "outputId": "86c0ed7e-63cf-4630-f396-de62d7f7b79a"
      },
      "source": [
        "# can save the pipeline\n",
        "\n",
        "fitted_model = fitted_pipeline.stages[0]\n",
        "print(fitted_model)\n",
        "\n",
        "fitted_model.save(\"fitted_pipeline\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ElephasTransformer_6fc2eefdd213\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}